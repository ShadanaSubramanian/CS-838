{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "A = em.read_csv_metadata('/Users/daravinds/Documents/Projects/CS-838/Stage4-IntegrationAndAnalysis/CS-838-bookdepository.csv',key='id')\n",
    "B = em.read_csv_metadata('/Users/daravinds/Documents/Projects/CS-838/Stage4-IntegrationAndAnalysis/CS-838-goodreads.csv',key='id') \n",
    "\n",
    "# A = A.replace(np.nan, '', regex=True)\n",
    "# B = B.replace(np.nan, '', regex=True)\n",
    "A['Language'] = A['Language'].fillna('')\n",
    "B['Language'] = B['Language'].fillna('')\n",
    "\n",
    "A['Publishing Date'] = A['Publishing Date'].fillna('')\n",
    "B['Publishing Date'] = B['Publishing Date'].fillna('')\n",
    "\n",
    "A['Publisher'] = A['Publisher'].fillna('')\n",
    "B['Publisher'] = B['Publisher'].fillna('')\n",
    "\n",
    "A['Genres'] = A['Genres'].fillna('')\n",
    "B['Genres'] = B['Genres'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_tuples = dict()\n",
    "for index, entry in A.iterrows():\n",
    "    A_tuples[entry['id']] = list([entry[\"Title\"], entry[\"Authors\"], entry[\"Genres\"], entry[\"Publishing Date\"], entry[\"Pages\"], entry[\"Publisher\"], entry[\"Language\"]])\n",
    "\n",
    "B_tuples = dict()\n",
    "for index, entry in B.iterrows():\n",
    "    B_tuples[entry['id']] = list([entry[\"Title\"], entry[\"Authors\"], entry[\"Genres\"], entry[\"Publishing Date\"], entry[\"Pages\"], entry[\"Publisher\"], entry[\"Language\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "I = em.read_csv_metadata(\"train_set.csv\", \n",
    "                         key='_id',\n",
    "                         ltable=A, rtable=B, \n",
    "                         fk_ltable='ltable_id', fk_rtable='rtable_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of features\n",
    "F = em.get_features_for_matching(A, B, validate_inferred_attr_types=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    }
   ],
   "source": [
    "# Convert the I into a set of feature vectors using F\n",
    "H = em.extract_feature_vecs(I, \n",
    "                            feature_table=F, \n",
    "                            attrs_after=['match'])\n",
    "\n",
    "# Impute feature vectors with the mean of the column values.\n",
    "H = em.impute_table(H, \n",
    "                exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'match'],\n",
    "                strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of ML-matchers\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "\n",
    "# Train using feature vectors from I \n",
    "lg.fit(table=H, \n",
    "       exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'match'], \n",
    "       target_attr='match')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = em.read_csv_metadata(\"after_blocking_tuples.csv\", \n",
    "                         key='_id',\n",
    "                         ltable=A, rtable=B, \n",
    "                         fk_ltable='ltable_id', fk_rtable='rtable_id')\n",
    "\n",
    "# Convert J into a set of feature vectors using F\n",
    "L = em.extract_feature_vecs(J, feature_table=F, show_progress=False)\n",
    "L = L.fillna(0)\n",
    "em.set_property(L, 'key', '_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on L \n",
    "predictions = lg.predict(table=L, exclude_attrs=['_id', 'ltable_id', 'rtable_id'], \n",
    "              append=True, target_attr='predicted', inplace=False, return_probs=True,\n",
    "                        probs_attr='proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "879"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(predictions['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predictions)\n",
    "matches_df = df.loc[df['predicted'] == 1]\n",
    "\n",
    "header = [\"L_Title\", \"L_Authors\", \"L_Genres\", \"L_Publishing Date\", \"L_Pages\", \"L_Publisher\", \"L_Language\", \"R_Title\", \"R_Authors\", \"R_Genres\", \"R_Publishing Date\", \"R_Pages\", \"R_Publisher\", \"Language\"]]\n",
    "with open(\"matches_A_B.csv\", 'a') as matches_file:\n",
    "    for index, a in matches_df.iterrows():\n",
    "        row1 = A_tuples[a['ltable_id']]\n",
    "        row2 = B_tuples[a['rtable_id']]\n",
    "        line = \",\".join(row1)\n",
    "        line += \",\"\n",
    "        line += \",\".join(row2)\n",
    "        line += \"\\n\"\n",
    "        matches_file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_map = dict()\n",
    "for index, match in matches_df.iterrows():\n",
    "    if match['ltable_id'] in match_map:\n",
    "        existing_matches = match_map.get(match['ltable_id'])\n",
    "    else:\n",
    "        existing_matches = set()\n",
    "    existing_matches.add(match['rtable_id'])\n",
    "    match_map[match['ltable_id']] = existing_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = set()\n",
    "for values in match_map.values():\n",
    "    for value in values:\n",
    "        all_values.add(value)\n",
    "\n",
    "A_ids = set(A['id'])\n",
    "A_matched_ids = set(match_map.keys())\n",
    "ltable_ids_without_match = A_ids.difference(A_matched_ids)\n",
    "\n",
    "B_ids = set(B['id'])\n",
    "B_matched_ids = all_values\n",
    "rtable_ids_without_match = B_ids.difference(B_matched_ids)\n",
    "\n",
    "merged_data = pd.DataFrame()\n",
    "data_from_A = pd.DataFrame(A.loc[A['id'].isin(ltable_ids_without_match)], columns=[\"Title\", \"Authors\", \"Genres\", \"Publishing Date\", \"Pages\", \"Publisher\", \"Language\"])\n",
    "data_from_B = pd.DataFrame(B.loc[B['id'].isin(rtable_ids_without_match)], columns=[\"Title\", \"Authors\", \"Genres\", \"Publishing Date\", \"Pages\", \"Publisher\", \"Language\"])\n",
    "merged_data = data_from_A.append(data_from_B, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_titles(records):\n",
    "    length = 0\n",
    "    for record in records:\n",
    "        cur_title = record['Title'].strip()\n",
    "        if len(cur_title) > length:\n",
    "            length = len(cur_title)\n",
    "            title = cur_title\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_authors(records):\n",
    "    authors = list()\n",
    "    length = 0\n",
    "    for record in records:\n",
    "        current_authors = record['Authors'].strip().split(\":\")\n",
    "        if len(current_authors) > length:\n",
    "            length = len(current_authors)\n",
    "            authors = \":\".join(current_authors)\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_genres(records):\n",
    "    genres = set(records[-1]['Genres'].strip().split(\":\"))\n",
    "    for i in range(len(records) - 1):\n",
    "        cur_genres = records[i]['Genres'].strip().split(\":\")\n",
    "        genres.union(cur_genres)\n",
    "\n",
    "    return \":\".join(list(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_publishing_dates(records):\n",
    "    for record in records:\n",
    "        pub_date = record['Publishing Date'].strip()\n",
    "        if pub_date is not None and len(pub_date) > 0:\n",
    "            return pub_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pages(records):\n",
    "    for record in records:\n",
    "        pages = record['Pages'].strip()\n",
    "        if pages is not None and len(pages) > 0:\n",
    "            return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_publishers(records):\n",
    "    for record in records:\n",
    "        publishers = record['Publisher'].strip()\n",
    "        if publishers is not None and len(publishers) > 0:\n",
    "            return publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_languages(records):\n",
    "    for record in records:\n",
    "        languages = record['Language'].strip()\n",
    "        if languages is not None and len(languages) > 0:\n",
    "            return languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tuples(records):\n",
    "    final_title = merge_titles(records)\n",
    "    authors = merge_authors(records)\n",
    "    genres = merge_genres(records)\n",
    "    publishing_date = merge_publishing_dates(records)\n",
    "    pages = merge_pages(records)\n",
    "    publisher = merge_publishers(records)\n",
    "    language = merge_languages(records)\n",
    "    final_record = (final_title, authors, genres, publishing_date, pages, publisher, language)\n",
    "\n",
    "    return final_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_map\n",
    "\n",
    "final_records = list()\n",
    "for l_id, r_ids in match_map.items():\n",
    "    records = list()\n",
    "    matches= B.loc[B['id'].isin(r_ids)]\n",
    "    for index, match in matches.iterrows():\n",
    "        records.append(match)\n",
    "    matches = A.loc[A['id'] == l_id]\n",
    "    for index, match in matches.iterrows():\n",
    "        records.append(match)\n",
    "\n",
    "    record = merge_tuples(records)\n",
    "    final_records.append(record)\n",
    "    \n",
    "final_df = pd.DataFrame(final_records, columns=[\"Title\", \"Authors\", \"Genres\", \"Publishing Date\", \"Pages\", \"Publisher\", \"Language\"])\n",
    "merged_data2 = merged_data.append(final_df, ignore_index = True)\n",
    "merged_data2.to_csv(\"unified_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7098"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_data2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS-838",
   "language": "python",
   "name": "entitymatching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
